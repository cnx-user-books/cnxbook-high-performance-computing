<document xmlns="http://cnx.rice.edu/cnxml">
  <title>Introduction</title>
<metadata xmlns:md="http://cnx.rice.edu/mdml">
  <md:content-id>m32775</md:content-id>
  <md:title>Introduction</md:title>
  <md:abstract/>
  <md:uuid>fc8f7eb2-d5f7-4cd8-a045-302590ca0979</md:uuid>
</metadata>
<featured-links>
  <!-- WARNING! The 'featured-links' section is read only. Do not edit below.
       Changes to the links section in the source will not be saved. -->
    <link-group type="supplemental">
      <link url="http://cnx.org/content/m32709/latest/" strength="3">Acknowledgements</link>
    </link-group>
  <!-- WARNING! The 'featured-links' section is read only. Do not edit above.
       Changes to the links section in the source will not be saved. -->
</featured-links>
<content>
      <para id="id20567186">In a sense, we have been talking about parallelism from the beginning of the book. Instead of calling it “parallelism,” we have been using words like “pipelined,” “superscalar,” and “compiler flexibility.” As we move into programming on multiprocessors, we must increase our understanding of parallelism in order to understand how to effectively program these systems. In short, as we gain more parallel resources, we need to find more parallelism in our code.</para>
      <para id="id11443341">When we talk of parallelism, we need to understand the concept of granularity. The granularity of parallelism indicates the size of the computations that are being performed at the same time between synchronizations. Some examples of parallelism in order of increasing grain size are:</para>
      <list id="id20524674" list-type="bulleted">
        <item>When performing a 32-bit integer addition, using a carry lookahead adder, you can partially add bits 0 and 1 at the same time as bits 2 and 3.</item>
        <item>On a pipelined processor, while decoding one instruction, you can fetch the next instruction.</item>
        <item>On a two-way superscalar processor, you can execute any combination of an integer and a floating-point instruction in a single cycle.</item>
        <item>On a multiprocessor, you can divide the iterations of a loop among the four processors of the system.</item>
        <item>You can split a large array across four workstations attached to a network. Each workstation can operate on its local information and then exchange boundary values at the end of each time step.</item>
      </list>
      <para id="id19670200">In this chapter, we start at <emphasis effect="italics">instruction-level parallelism</emphasis> (pipelined and superscalar) and move toward <emphasis effect="italics">thread-level parallelism</emphasis>, which is what we need for multiprocessor systems. It is important to note that the different levels of parallelism are generally not in conflict. Increasing thread parallelism at a coarser grain size often exposes more fine-grained parallelism.</para>
      <para id="id11361836">The following is a loop that has plenty of parallelism:</para>
      <code id="id19672348" display="block"><newline/>
      DO I=1,16000
        A(I) = B(I) * 3.14159
      ENDDO
    </code>
      <para id="id19695314">We have expressed the loop in a way that would imply that A(1) must be computed first, followed by A(2), and so on. However, once the loop was completed, it would not have mattered if A(16000), were computed first followed by A(15999), and so on. The loop could have computed the even values of I and then computed the odd values of I. It would not even make a difference if all 16,000 of the iterations were computed simultaneously using a 16,000-way superscalar processor.<footnote id="id19664174">Interestingly, this is not as far-fetched as it might seem. On a single instruction multiple data (SIMD) computer such as the Connection CM-2 with 16,384 processors, it would take three instruction cycles to process this entire loop.</footnote> If the compiler has flexibility in the order in which it can execute the instructions that make up your program, it can execute those instructions simultaneously when parallel hardware is available.</para>
      <para id="id17147361">One technique that computer scientists use to formally analyze the potential parallelism in an algorithm is to characterize how quickly it would execute with an “infinite-way” superscalar processor.</para>
      <para id="id20238978">Not all loops contain as much parallelism as this simple loop. We need to identify the things that limit the parallelism in our codes and remove them whenever possible. In previous chapters we have already looked at removing clutter and rewriting loops to simplify the body of the loop.</para>
      <para id="id19696093">This chapter also supplements <link document="m33690"/>, in many ways. We looked at the mechanics of compiling code, all of which apply here, but we didn’t answer all of the “whys.” Basic block analysis techniques form the basis for the work the compiler does when looking for more parallelism. Looking at two pieces of data, instructions, or data and instructions, a modern compiler asks the question, “Do these things depend on each other?” The three possible answers are yes, no, and we don’t know. The third answer is effectively the same as a yes, because a compiler has to be conservative whenever it is unsure whether it is safe to tweak the ordering of instructions.</para>
      <para id="id19692788">Helping the compiler recognize parallelism is one of the basic approaches specialists take in tuning code. A slight rewording of a loop or some supplementary information supplied to the compiler can change a “we don’t know” answer into an opportunity for parallelism. To be certain, there are other facets to tuning as well, such as optimizing memory access patterns so that they best suit the hardware, or recasting an algorithm. And there is no single best approach to every problem; any tuning effort has to be a combination of techniques.</para>
  </content>
</document>
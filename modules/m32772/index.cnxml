<document xmlns="http://cnx.rice.edu/cnxml" xmlns:m="http://www.w3.org/1998/Math/MathML">
  <title>Representation</title>
<metadata xmlns:md="http://cnx.rice.edu/mdml">
  <md:content-id>m32772</md:content-id>
  <md:title>Representation</md:title>
  <md:abstract/>
  <md:uuid>52683112-fa72-4933-8f09-d9618d7df295</md:uuid>
</metadata>
<featured-links>
  <!-- WARNING! The 'featured-links' section is read only. Do not edit below.
       Changes to the links section in the source will not be saved. -->
    <link-group type="supplemental">
      <link url="http://cnx.org/content/m32709/latest/" strength="3">Acknowledgements</link>
    </link-group>
  <!-- WARNING! The 'featured-links' section is read only. Do not edit above.
       Changes to the links section in the source will not be saved. -->
</featured-links>
<content>
      <para id="id20063906">Given that we cannot perfectly represent real numbers on digital computers, we must come up with a compromise that allows us to approximate real numbers.<footnote id="id21104485">Interestingly, analog computers have an easier time representing real numbers. Imagine a “water- adding” analog computer which consists of two glasses of water and an empty glass. The amount of water in the two glasses are perfectly represented real numbers. By pouring the two glasses into a third, we are adding the two real numbers perfectly (unless we spill some), and we wind up with a real number amount of water in the third glass. The problem with analog computers is knowing just how much water is in the glasses when we are all done. It is also problematic to perform 600 million additions per second using this technique without getting pretty wet. Try to resist the temptation to start an argument over whether quantum mechanics would cause the real numbers to be rational numbers. And don’t point out the fact that even digital computers are really analog computers at their core. I am trying to keep the focus on floating-point values, and you keep drifting away!</footnote> There are a number of different ways that have been used to represent real numbers. The challenge in selecting a representation is the trade-off between space and accuracy and the tradeoff between speed and accuracy. In the field of high performance computing we generally expect our processors to produce a floating- point result every 600-MHz clock cycle. It is pretty clear that in most applications we aren’t willing to drop this by a factor of 100 just for a little more accuracy. Before we discuss the format used by most high performance computers, we discuss some alternative (albeit slower) techniques for representing real numbers.</para>
      <section id="id18243594">
        <title>Binary Coded Decimal</title>
        <para id="id15124125">In the earliest computers, one technique was to use binary coded decimal (BCD). In BCD, each base-10 digit was stored in four bits. Numbers could be arbitrarily long with as much precision as there was memory:</para>
        <code id="id18936197" display="block">123.45
0001 0010 0011 0100 0101
    </code>
        <para id="id15222678">This format allows the programmer to choose the precision required for each variable. Unfortunately, it is difficult to build extremely high-speed hardware to perform arithmetic operations on these numbers. Because each number may be far longer than 32 or 64 bits, they did not fit nicely in a register. Much of the floating- point operations for BCD were done using loops in microcode. Even with the flexibility of accuracy on BCD representation, there was still a need to round real numbers to fit into a limited amount of space.</para>
        <para id="id9373821">Another limitation of the BCD approach is that we store a value from 0–9 in a four-bit field. This field is capable of storing values from 0–15 so some of the space is wasted.</para>
      </section>
      <section id="id18924103">
        <title>Rational Numbers</title>
        <para id="id12193310">One intriguing method of storing real numbers is to store them as rational numbers. To briefly review mathematics, rational numbers are the subset of real numbers that can be expressed as a ratio of integer numbers. For example, 22/7 and 1/2 are rational numbers. Some rational numbers, such as 1/2 and 1/10, have perfect representation as base-10 decimals, and others, such as 1/3 and 22/7, can only be expressed as infinite-length base-10 decimals. When using rational numbers, each real number is stored as two integer numbers representing the numerator and denominator. The basic fractional arithmetic operations are used for addition, subtraction, multiplication, and division, as shown in <link target-id="id18527915"/>.</para>
        <figure id="id18527915"><title>Rational number mathematics</title><media id="id18527915_media" alt="This figure shows three rows of equations. The first is one third times thirty sevenths equals thirty twenty-firsts, which equals ten sevenths. The second is one sixth plus one fifth equals five thirtieths plus six thirtieths, which equals eleven thirtieths. The third is the quantity fourteen thousand one hundred seventy three divided by twenty-one thousand two-hundred twenty four times the quantity seventy seven thousand two hundred thirty four divided by two thousand one hundred twenty one, which equals the quantity one billion, ninety-four million, six hundred thirty seven thousand, four hundred eighty two divided by forty-five million, sixteen thousand, one hundred four, which equals the quantity five hundred forty seven million, three hundred eighteen thousand, seven hundred forty one divided by twenty-two million, five hundred eight thousand fifty two.">
            <image mime-type="image/png" src="../../media/graphics1-5306.png" id="id18527915__onlineimage" height="245" width="600"/>
          </media>
        </figure>
        
        <para id="id18627899">The limitation that occurs when using rational numbers to represent real numbers is that the size of the numerators and denominators tends to grow. For each addition, a common denominator must be found. To keep the numbers from becoming extremely large, during each operation, it is important to find the <emphasis effect="italics">greatest common divisor</emphasis> (GCD) to reduce fractions to their most compact representation. When the values grow and there are no common divisors, either the large integer values must be stored using dynamic memory or some form of approximation must be used, thus losing the primary advantage of rational numbers.</para>
        <para id="id16826560">For mathematical packages such as Maple or Mathematica that need to produce exact results on smaller data sets, the use of rational numbers to represent real numbers is at times a useful technique. The performance and storage cost is less significant than the need to produce exact results in some instances.</para>
      </section>
      <section id="id10086969">
        <title>Fixed Point</title>
        <para id="id18205428">If the desired number of decimal places is known in advance, it’s possible to use fixed-point representation. Using this technique, each real number is stored as a scaled integer. This solves the problem that base-10 fractions such as 0.1 or 0.01 cannot be perfectly represented as a base-2 fraction. If you multiply 110.77 by 100 and store it as a scaled integer 11077, you can perfectly represent the base-10 fractional part (0.77). This approach can be used for values such as money, where the number of digits past the decimal point is small and known.</para>
        <para id="id16858933">However, just because all numbers can be accurately represented it doesn’t mean there are not errors with this format. When multiplying a fixed-point number by a fraction, you get digits that can’t be represented in a fixed-point format, so some form of rounding must be used. For example, if you have $125.87 in the bank at 4% interest, your interest amount would be $5.0348. However, because your bank balance only has two digits of accuracy, they only give you $5.03, resulting in a balance of $130.90. Of course you probably have heard many stories of programmers getting rich depositing many of the remaining 0.0048 amounts into their own account. My guess is that banks have probably figured that one out by now, and the bank keeps the money for itself. But it does make one wonder if they round or truncate in this type of calculation.<footnote id="id19205337">Perhaps banks round this instead of truncating, knowing that they will always make it up in teller machine fees.</footnote></para>
      </section>
      <section id="id13510063">
        <title>Mantissa/Exponent</title>
        <para id="id18196007">The floating-point format that is most prevalent in high performance computing is a variation on scientific notation. In scientific notation the real number is represented using a mantissa, base, and exponent: 6.02 × 10<sup>23</sup>.</para>
        <para id="id14449260">The mantissa typically has some fixed number of places of accuracy. The mantissa can be represented in base 2, base 16, or BCD. There is generally a limited range of exponents, and the exponent can be expressed as a power of 2, 10, or 16.</para>
        <para id="id11240194">The primary advantage of this representation is that it provides a wide overall range of values while using a fixed-length storage representation. The primary limitation of this format is that the difference between two successive values is not uniform. For example, assume that you can represent three base-10 digits, and your exponent can range from –10 to 10. For numbers close to zero, the “distance” between successive numbers is very small. For the number <m:math>
<m:mn>1.72</m:mn>
<m:mo>×</m:mo>
<m:msup>
<m:mn>10</m:mn>
<m:mn>-10</m:mn>
</m:msup>
</m:math>, the next larger number is <m:math>
<m:mn>1.73</m:mn>
<m:mo>×</m:mo>
<m:msup>
<m:mn>10</m:mn>
<m:mn>-10</m:mn>
</m:msup>
</m:math>. The distance between these two “close” small numbers is 0.000000000001. For the number <m:math>
<m:mn>6.33</m:mn>
<m:mo>×</m:mo>
<m:msup>
<m:mn>10</m:mn>
<m:mn>10</m:mn>
</m:msup>
</m:math>, the next larger number is <m:math>
<m:mn>6.34</m:mn>
<m:mo>×</m:mo>
<m:msup>
<m:mn>10</m:mn>
<m:mn>10</m:mn>
</m:msup>
</m:math>. The distance between these “close” large numbers is 100 million.</para>
        <para id="id16275214">In <link target-id="id16863633"/>, we use two base-2 digits with an exponent ranging from –1 to 1.</para>
        <para id="id12972414">
          <figure id="id16863633"><title>Distance between successive floating-point numbers</title><media id="id16863633_media" alt="This figure is a horizontal line with labeled hash-marks at various distances From left to right, the hash marks read 0.0 times 2^-1, 0.1 times 2^-1, 1.0 times 2^-1, 1.1 times 2^-1, 1.0 times 2^0, 1.1 times 2^0, 1.0 times 2^1, and 1.1 times 2^1.">
              <image mime-type="image/png" src="../../media/graphics2-095d.png" id="id16863633__onlineimage" height="129" width="600"/>
            </media>
          </figure>
        </para>
        
        <para id="id13298345">There are multiple equivalent representations of a number when using scientific notation:</para>
        <para id="id19173975"><m:math><m:mrow><m:mn>6.00</m:mn><m:mo>×</m:mo><m:msup><m:mn>10</m:mn><m:mn>5</m:mn></m:msup></m:mrow></m:math><newline/>
<m:math><m:mrow><m:mn>0.60</m:mn><m:mo>×</m:mo><m:msup><m:mn>10</m:mn><m:mn>6</m:mn></m:msup></m:mrow></m:math><newline/>
<m:math><m:mrow><m:mn>0.06</m:mn><m:mo>×</m:mo><m:msup><m:mn>10</m:mn><m:mn>7</m:mn></m:msup></m:mrow></m:math></para>
        
        
        <para id="id12280316">By convention, we shift the mantissa (adjust the exponent) until there is exactly one nonzero digit to the left of the decimal point. When a number is expressed this way, it is said to be “normalized.” In the above list, only 6.00 × 10<sup>5</sup> is normalized. <link target-id="id17974284"/> shows how some of the floating-point numbers from <link target-id="id16863633"/> are not normalized.</para>
        <para id="id18993576">While the mantissa/exponent has been the dominant floating-point approach for high performance computing, there were a wide variety of specific formats in use by computer vendors. Historically, each computer vendor had their own particular format for floating-point numbers. Because of this, a program executed on several different brands of computer would generally produce different answers. This invariably led to heated discussions about which system provided the right answer and which system(s) were generating meaningless results.<footnote id="id15940697">Interestingly, there was an easy answer to the question for many programmers. Generally they trusted the results from the computer they used to debug the code and dismissed the results from other computers as garbage.</footnote></para>
        <figure id="id17974284"><title>Normalized floating-point numbers</title><media id="id17974284_media" alt="This figure is a horizontal line with labeled hash-marks at various distances From left to right, the hash marks read 0.0 times 2^-1, 0.1 times 2^-1, 1.0 times 2^-1, 1.1 times 2^-1, 1.0 times 2^0, 1.1 times 2^0, 1.0 times 2^1, and 1.1 times 2^1. Pointing at the first two hash marks with two arrows is the label, not normalized.">
            <image mime-type="image/png" src="../../media/graphics3-6dda.png" id="id17974284__onlineimage" height="173" width="600"/>
          </media>
        </figure>
        
        <para id="id8862462">When storing floating-point numbers in digital computers, typically the mantissa is normalized, and then the mantissa and exponent are converted to base-2 and packed into a 32- or 64-bit word. If more bits were allocated to the exponent, the overall range of the format would be increased, and the number of digits of accuracy would be decreased. Also the base of the exponent could be base-2 or base-16. Using 16 as the base for the exponent increases the overall range of exponents, but because normalization must occur on four-bit boundaries, the available digits of accuracy are reduced on the average. Later we will see how the IEEE 754 standard for floating-point format represents numbers.</para>
      </section>
  </content>
</document>
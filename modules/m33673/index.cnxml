<document xmlns="http://cnx.rice.edu/cnxml">
  <title>Fundamental of RISC</title>
<metadata xmlns:md="http://cnx.rice.edu/mdml">
  <md:content-id>m33673</md:content-id>
  <md:title>Fundamental of RISC</md:title>
  <md:abstract/>
  <md:uuid>818b2af4-c1a1-422e-9c47-b5c2fb0f8d9a</md:uuid>
</metadata>

<content>
      <para id="id1171015642583">A RISC machine could have been built in 1960. (In fact, Seymour Cray built one in 1964 — the CDC 6600.) However, given the same costs of components, technical barriers, and even expectations for how computers would be used, you would probably still have chosen a CISC design — even with the benefit of hindsight.</para>
      <para id="id1171015575016">The exact inspiration that led to developing high performance RISC microprocessors in the 1980s is a subject of some debate. Regardless of the motivation of the RISC designers, there were several obvious pressures that affected the development of RISC:</para>
      <list id="id1171017847513" list-type="bulleted">
        <item>The number of transistors that could fit on a single chip was increasing. It was clear that one would eventually be able to fit all the components from a processor board onto a single chip.</item>
        <item>Techniques such as pipelining were being explored to improve performance. Variable-length instructions and variable-length instruction execution times (due to varying numbers of microcode steps) made implementing pipelines more difficult.</item>
        <item>As compilers improved, they found that well-optimized sequences of stream- lined instructions often outperformed the equivalent complicated multi-cycle instructions. (See Appendix A, Processor Architectures, and Appendix B, Looking at Assembly Language.)</item>
      </list>
      <para id="id1171020950964">The RISC designers sought to create a high performance single-chip processor with a fast clock rate. When a CPU can fit on a single chip, its cost is decreased, its reliability is increased, and its clock speed can be increased. While not all RISC processors are single-chip implementation, most use a single chip.</para>
      <para id="id8603708">To accomplish this task, it was necessary to discard the existing CISC instruction sets and develop a new minimal instruction set that could fit on a single chip. Hence the term <emphasis effect="italics">reduced instruction set computer</emphasis>. In a sense reducing the instruction set was not an “end” but a means to an end.</para>
      <para id="id1171022745127">For the first generation of RISC chips, the restrictions on the number of components that could be manufactured on a single chip were severe, forcing the designers to leave out hardware support for some instructions. The earliest RISC processors had no floating-point support in hardware, and some did not even support integer multiply in hardware. However, these instructions could be implemented using software routines that combined other instructions (a microcode of sorts).</para>
      <para id="id1171015005008">These earliest RISC processors (most severely reduced) were not overwhelming successes for four reasons:</para>
      <list id="id6954953" list-type="bulleted">
        <item>It took time for compilers, operating systems, and user software to be retuned to take advantage of the new processors.</item>
        <item>If an application depended on the performance of one of the software-implemented instructions, its performance suffered dramatically.</item>
        <item>Because RISC instructions were simpler, more instructions were needed to accomplish the task.</item>
        <item>Because all the RISC instructions were 32 bits long, and commonly used CISC instructions were as short as 8 bits, RISC program executables were often larger.</item>
      </list>
      <para id="id1171022221450">As a result of these last two issues, a RISC program may have to fetch more memory for its instructions than a CISC program. This increased appetite for instructions actually clogged the memory bottleneck until sufficient caches were added to the RISC processors. In some sense, you could view the caches on RISC processors as the microcode store in a CISC processor. Both reduced the overall appetite for instructions that were loaded from memory.</para>
      <para id="id1171020904056">While the RISC processor designers worked out these issues and the manufacturing capability improved, there was a battle between the existing (now called CISC) processors and the new RISC (not yet successful) processors. The CISC processor designers had mature designs and well-tuned popular software. They also kept adding performance tricks to their systems. By the time Motorola had evolved from the MC68000 in 1982 that was a CISC processor to the MC68040 in 1989, they referred to the MC68040 as a RISC processor.<footnote id="id1171017017577">And they did it without ever taking out a single instruction!</footnote></para>
      <para id="id1171015607713">However, the RISC processors eventually became successful. As the amount of logic available on a single chip increased, floating-point operations were added back onto the chip. Some of the additional logic was used to add on-chip cache to solve some of the memory bottleneck problems due to the larger appetite for instruction memory. These and other changes moved the RISC architectures from the defensive to the offensive.</para>
      <para id="id1171016631503">RISC processors quickly became known for their affordable high-speed floating- point capability compared to CISC processors.<footnote id="id1171017868342">The typical CISC microprocessor in the 1980s supported floating-point operations in a separate coprocessor.</footnote> This excellent performance on scientific and engineering applications effectively created a new type of computer system, the workstation. Workstations were more expensive than personal computers but their cost was sufficiently low that workstations were heavily used in the CAD, graphics, and design areas. The emerging workstation market effectively created three new computer companies in Apollo, Sun Microsystems, and Silicon Graphics.</para>
      <para id="id1171015871347">Some of the existing companies have created competitive RISC processors in addition to their CISC designs. IBM developed its RS-6000 (RIOS) processor, which had excellent floating-point performance. The Alpha from DEC has excellent performance in a number of computing benchmarks. Hewlett-Packard has developed the PA-RISC series of processors with excellent performance. Motorola and IBM have teamed to develop the PowerPC series of RISC processors that are used in IBM and Apple systems.</para>
      <para id="id1171015711406">By the end of the RISC revolution, the performance of RISC processors was so impressive that single and multiprocessor RISC-based server systems quickly took over the minicomputer market and are currently encroaching on the traditional mainframe market.</para>
      <section id="id1171016095951">
        <title>Characterizing RISC</title>
        <para id="id3448291">RISC is more of a design philosophy than a set of goals. Of course every RISC processor has its own personality. However, there are a number of features commonly found in machines people consider to be RISC:</para>
        <list id="id1171019077969" list-type="bulleted">
          <item>Instruction pipelining</item>
          <item>Pipelining floating-point execution </item>
          <item>Uniform instruction length</item>
          <item>Delayed branching</item>
          <item>Load/store architecture</item>
          <item>Simple addressing modes</item>
        </list>
        <para id="id6032848">This list highlights the differences between RISC and CISC processors. Naturally, the two types of instruction-set architectures have much in common; each uses registers, memory, etc. And many of these techniques are used in CISC machines too, such as caches and instruction pipelines. It is the fundamental differences that give RISC its speed advantage: focusing on a smaller set of less powerful instructions makes it possible to build a faster computer.</para>
        <para id="id1171017272395">However, the notion that RISC machines are generally simpler than CISC machines isn’t correct. Other features, such as functional pipelines, sophisticated memory systems, and the ability to issue two or more instructions per clock make the latest RISC processors the most complicated ever built. Furthermore, much of the complexity that has been lifted from the instruction set has been driven into the compilers, making a good optimizing compiler a prerequisite for machine performance.</para>
        <para id="id8884646">Let’s put ourselves in the role of computer architect again and look at each item in the list above to understand why it’s important.</para>
      </section>
      <section id="id1171015065654">
        <title>Pipelines</title>
        <para id="id1171015704856">Everything within a digital computer (RISC or CISC) happens in step with a <emphasis effect="italics">clock</emphasis>: a signal that paces the computer’s circuitry. The rate of the clock, or <emphasis effect="italics">clock speed</emphasis>, determines the overall speed of the processor. There is an upper limit to how fast you can clock a given computer.</para>
        <para id="id1171022726668">A number of parameters place an upper limit on the clock speed, including the semiconductor technology, packaging, the length of wires tying the pieces together, and the longest path in the processor. Although it may be possible to reach blazing speed by optimizing all of the parameters, the cost can be prohibitive. Furthermore, exotic computers don’t make good office mates; they can require too much power, produce too much noise and heat, or be too large. There is incentive for manufacturers to stick with manufacturable and marketable technologies.</para>
        <para id="id1171016532017">Reducing the number of clock ticks it takes to execute an individual instruction is a good idea, though cost and practicality become issues beyond a certain point. A greater benefit comes from partially overlapping instructions so that more than one can be in progress simultaneously. For instance, if you have two additions to perform, it would be nice to execute them both at the same time. How do you do that? The first, and perhaps most obvious, approach, would be to start them simultaneously. Two additions would execute together and complete together in the amount of time it takes to perform one. As a result, the throughput would be effectively doubled. The downside is that you would need hardware for two adders in a situation where space is usually at a premium (especially for the early RISC processors).</para>
        <para id="id1171017681444">Other approaches for overlapping execution are more cost-effective than side-by-side execution. Imagine what it would be like if, a moment after launching one operation, you could launch another without waiting for the first to complete. Perhaps you could start another of the same type right behind the first one — like the two additions. This would give you nearly the performance of side-by-side execution without duplicated hardware. Such a mechanism does exist to varying degrees in all computers — CISC and RISC. It’s called a pipeline. A pipeline takes advantage of the fact that many operations are divided into identifiable steps, each of which uses different resources on the processor.<footnote id="id1171022570505">Here is a simple analogy: imagine a line at a fast-food drive up window. If there is only one window, one customer orders and pays, and the food is bagged and delivered to the customer before the second customer orders. For busier restaurants, there are three windows. First you order, then move ahead. Then at a second window, you pay and move ahead. At the third window you pull up, grab the food and roar off into the distance. While your wait at the three-window (pipelined) drive-up may have been slightly longer than your wait at the one-window (non-pipelined) restaurant, the pipeline solution is significantly better because multiple customers are being processed simultaneously.</footnote></para>
        <figure id="id1171015683047"><title>A Pipeline</title><media id="id1171015683047_media" alt="Figure displays an arrow labeled, instructions in, pointing to the right, at 5 boxes numbered sequentially, labeled stage, followed by an arrow to the right pointing to the right, labeled results out.">
            <image mime-type="image/png" src="../../media/Picture 1-ac57.png" id="id1171015683047__onlineimage" height="144" width="700"/>
          </media>
        </figure>
        <para id="id1171015058109"><link target-id="id1171015683047"/> shows a conceptual diagram of a pipeline. An operation entering at the left proceeds on its own for five clock ticks before emerging at the right. Given that the pipeline stages are independent of one another, up to five operations can be in flight at a time as long as each instruction is delayed long enough for the previous instruction to clear the pipeline stage. Consider how powerful this mechanism is: where before it would have taken five clock ticks to get a single result, a pipeline produces as much as one result every clock tick.</para>
        <para id="id1171015642354">Pipelining is useful when a procedure can be divided into stages. Instruction processing fits into that category. The job of retrieving an instruction from memory, figuring out what it does, and doing it are separate steps we usually lump together when we talk about executing an instruction. The number of steps varies, depending on whose processor you are using, but for illustration, let’s say there are five:</para>
        <list id="id1171015042488" list-type="enumerated" number-style="arabic">
          <item><emphasis effect="italics">Instruction fetch</emphasis>: The processor fetches an instruction from memory.</item>
          <item><emphasis effect="italics">Instruction decode</emphasis>: The instruction is recognized or decoded.</item>
          <item><emphasis effect="italics">Operand Fetch</emphasis>: The processor fetches the operands the instruction needs. These operands may be in registers or in memory.</item>
          <item><emphasis effect="italics">Execute</emphasis>: The instruction gets executed.</item>
          <item><emphasis effect="italics">Writeback</emphasis>: The processor writes the results back to wherever they are supposed to go —possibly registers, possibly memory.</item>
        </list>
        <para id="id1171015525408">Ideally, instruction </para>
        <para id="id1171015710946"><list id="eip-id15283526" list-type="enumerated" number-style="arabic"><item>Will be entering the operand fetch stage as instruction</item>
<item>enters instruction decode stage and instruction</item>
<item>starts instruction fetch, and so on.</item></list></para>
        
        
        <para id="id1171017012289">Our pipeline is five stages deep, so it should be possible to get five instructions in flight all at once. If we could keep it up, we would see one instruction complete per clock cycle.</para>
        <para id="id1171015548031">Simple as this illustration seems, instruction pipelining is complicated in real life. Each step must be able to occur on different instructions simultaneously, and delays in any stage have to be coordinated with all those that follow. In <link target-id="id1171023405110"/> we see three instructions being executed simultaneously by the processor, with each instruction in a different stage of execution.</para>
        <figure id="id1171023405110"><title>Three instructions in flight through one pipeline</title><media id="id1171023405110_media" alt="This figure shows three rows of instructions laid on three layers of a cartesian graph. The horizontal axis of the graph is labeled, Time. There is a vertical line placed three horizontal units to the right of the vertical axis. Starting one unit to the right of the vertical axis and two units above the horizontal axis is the first row of instructions. Below this, one space above the horizontal axis and two spaces to the right of the vertical axis is the second set of instructions. The third row of instructions happens along the horizontal axis and three spaces to the right of the vertical axis. Each set of instructions is a set of 5 connected boxes, numbered sequentially. Box 1 is labeled Ins. Fetch, Box 2 is labeled Ins. Decode, Box 3 is labeled Op. Fetch, Box 4 is labeled Exec, and Box 5 is labeled Writeback.">
            <image mime-type="image/png" src="../../media/graphics1-d6e0.png" id="id1171023405110__onlineimage" height="377" width="699"/>
          </media>
        </figure>
        <para id="id1171015057875">For instance, if a complicated memory access occurs in stage three, the instruction needs to be delayed before going on to stage four because it takes some time to calculate the operand’s address and retrieve it from memory. All the while, the rest of the pipeline is stalled. A simpler instruction, sitting in one of the earlier stages, can’t continue until the traffic ahead clears up.</para>
        <para id="id1171015659148">Now imagine how a jump to a new program address, perhaps caused by an if statement, could disrupt the pipeline flow. The processor doesn’t know an instruction is a branch until the decode stage. It usually doesn’t know whether a branch will be taken or not until the execute stage. As shown in <link target-id="id6140934"/>, during the four cycles after the branch instruction was fetched, the processor blindly fetches instructions sequentially and starts these instructions through the pipeline.</para>
        <figure id="id6140934"><title>Detecting a branch</title><media id="id6140934_media" alt="This figure shows six rows of labeled boxes, reading from left to right, fetch, decode, operand, exec, and write. Each row begins one box to the right of the row above it, but retains the same order of labels. There is an arrow pointing from the first row's Exec label to the fifth row's Fetch label, and below is the caption Sure. Below the Fetch label in the first row are three unboxed rows of the word Guess, which are aligned evenly with the second, third, and fourth rows. In the upper-right portion of the graph is the caption, Branch.">
            <image mime-type="image/png" src="../../media/graphics2-4393.png" id="id6140934__onlineimage" height="347" width="699"/>
          </media>
        </figure>
        <para id="id1171015700941">If the branch “falls through,” then everything is in great shape; the pipeline simply executes the next instruction. It’s as if the branch were a “no-op” instruction. However, if the branch jumps away, those three partially processed instructions never get executed. The first order of business is to discard these “in-flight” instructions from the pipeline. It turns out that because none of these instructions was actually going to do anything until its execute stage, we can throw them away without hurting anything (other than our efficiency). Somehow the processor has to be able to clear out the pipeline and restart the pipeline at the branch destination.</para>
        <para id="id1171015095615">Unfortunately, branch instructions occur every five to ten instructions in many programs. If we executed a branch every fifth instruction and only half our branches fell through, the lost efficiency due to restarting the pipeline after the branches would be 20 percent.</para>
        <para id="id1171015868202">You need optimal conditions to keep the pipeline moving. Even in less-than-optimal conditions, instruction pipelining is a big win — especially for RISC processors. Interestingly, the idea dates back to the late 1950s and early 1960s with the UNI- VAC LARC and the IBM Stretch. Instruction pipelining became mainstreamed in 1964, when the CDC 6600 and the IBM S/360 families were introduced with pipelined instruction units — on machines that represented RISC-ish and CISC designs, respectively. To this day, ever more sophisticated techniques are being applied to instruction pipelining, as machines that can overlap instruction execution become commonplace.</para>
      </section>
      <section id="id1171015501560">
        <title>Pipelined Floating-Point Operations</title>
        <para id="id1171015056128">Because the execution stage for floating-point operations can take longer than the execution stage for fixed-point computations, these operations are typically pipelined, too. Generally, this includes floating-point addition, subtraction, multiplication, comparisons, and conversions, though it might not include square roots and division. Once a pipelined floating-point operation is started, calculations continue through the several stages without delaying the rest of the processor. The result appears in a register at some point in the future.</para>
        <para id="id1171016230455">Some processors are limited in the amount of overlap their floating-point pipelines can support. Internal components of the pipelines may be shared (for adding, multiplying, normalizing, and rounding intermediate results), forcing restrictions on when and how often you can begin new operations. In other cases, floating- point operations can be started every cycle regardless of the previous floating- point operations. We say that such operations are <emphasis effect="italics">fully pipelined</emphasis>.</para>
        <para id="id1171015484737">The number of stages in floating-point pipelines for affordable computers has decreased over the last 10 years. More transistors and newer algorithms make it possible to perform a floating-point addition or multiplication in just one to three cycles. Generally the most difficult instruction to perform in a single cycle is the floating-point multiply. However, if you dedicate enough hardware to it, there are designs that can operate in a single cycle at a moderate clock rate.</para>
      </section>
      <section id="id1171016490983">
        <title>Uniform Instruction Length</title>
        <para id="id1171021777042">Our sample instruction pipeline had five stages: instruction fetch, instruction decode, operand fetch, execution, and writeback. We want this pipeline to be able to process five instructions in various stages without stalling. Decomposing each operation into five identifiable parts, each of which is roughly the same amount of time, is challenging enough for a RISC computer. For a designer working with a CISC instruction set, it’s especially difficult because CISC instructions come in varying lengths. A simple “return from subroutine” instruction might be one byte long, for instance, whereas it would take a longer instruction to say “add register four to memory location 2005 and leave the result in register five.” The number of bytes to be fetched must be known by the fetch stage of the pipeline as shown in <link target-id="id1171020548664"/>.</para>
        <figure id="id1171020548664"><title>Variable length instructions make pipelining difficult</title><media id="id1171020548664_media" alt="This figure is a row of 5 connected boxes, numbered sequentially, and labeled Ins Fetch, Ins Decode, Exec, and Writeback. The first box is grey. Below the row is an arrow pointing at the first box, labeled, How many bytes should be fetched?">
            <image mime-type="image/png" src="../../media/graphics3-c8ab.png" id="id1171020548664__onlineimage" height="188" width="699"/>
          </media>
        </figure>
        <para id="id1171019982798">The processor has no way of knowing how long an instruction will be until it reaches the decode stage and determines what it is. If it turns out to be a long instruction, the processor may have to go back to memory and get the portion left behind; this stalls the pipeline. We could eliminate the problem by requiring that all instructions be the same length, and that there be a limited number of instruction formats as shown in <link target-id="id1171020371467"/>. This way, every instruction entering the pipeline is known <emphasis effect="italics">a priori</emphasis> to be complete — not needing another memory access. It would also be easier for the processor to locate the instruction fields that specify registers or constants. Altogether because RISC can assume a fixed instruction length, the pipeline flows much more smoothly.</para>
        <para id="id1171019036049">
          <figure id="id1171020371467"><title>Variable-length CISC versus fixed-length RISC instructions</title><media id="id1171020371467_media" alt="This figure is comprised of two diagrams. The diagram on the left, labeled CISC shows two boxes with the same height but variable widths. The first box, which is longer than the second, is labeled R1 arrow pointing to the left, [addr(A) + 4 * R2] + 1. The second is labeled return. Below the boxes are two arrows showing widths, labeled variable length. The second diagram is labeled RISC, and shows five connected boxes of equal size. The first reads R3, arrow pointing to the left, 4 * R2. The second reads R3, arrow pointing to the left, addr (A) + R3. The third reads R5, arrow pointing to the left, [R4]. The fourth reads R1, arrow pointing to the left, R5 + 1. The fifth is labeled return. Below the boxes is a set of arrows showing the length of all five boxes to be fixed.">
              <image mime-type="image/png" src="../../media/graphics4-eb1e.png" id="id1171020371467__onlineimage" height="298" width="700"/>
            </media>
          </figure>
        </para>
      </section>
      <section id="id1171021892725">
        <title>Delayed Branches</title>
        <para id="id1171019463112">As described earlier, branches are a significant problem in a pipelined architecture. Rather than take a penalty for cleaning out the pipeline after a misguessed branch, many RISC designs require an instruction after the branch. This instruction, in what is called the <emphasis effect="italics">branch delay slot</emphasis>, is executed no matter what way the branch goes. An instruction in this position should be useful, or at least harmless, whichever way the branch proceeds. That is, you expect the processor to execute the instruction following the branch in either case, and plan for it. In a pinch, a no-op can be used. A slight variation would be to give the processor the ability to <emphasis effect="italics">annul</emphasis> (or squash) the instruction appearing in the branch delay slot if it turns out that it shouldn’t have been issued after all:</para>
        
        <code id="eip-963" display="block"><newline/>
      ADD      R1,R2,R1          add r1 to r2 and store in r1
      SUB      R3,R1,R3          subtract r1 from r3, store in r3
      BRA      SOMEWHERE         branch somewhere else
      LABEL1   ZERO R3           instruction in branch delay slot
                ...</code><para id="id1171021578856">While branch delay slots appeared to be a very clever solution to eliminating pipeline stalls associated with branch operations, as processors moved toward exe- cuting two and four instructions simultaneously, another approach was needed.<footnote id="id1171021791853">Interestingly, while the delay slot is no longer critical in processors that execute four instructions simultaneously, there is not yet a strong reason to remove the feature. Removing the delay slot would be nonupwards-compatible, breaking many existing codes. To some degree, the branch delay slot has become “baggage” on those “new” 10-year-old architectures that must continue to support it.</footnote></para>
        <para id="id1171019506062">A more robust way of eliminating pipeline stalls was to “predict” the direction of the branch using a table stored in the decode unit. As part of the decode stage, the CPU would notice that the instruction was a branch and consult a table that kept the recent behavior of the branch; it would then make a guess. Based on the guess, the CPU would immediately begin fetching at the predicted location. As long as the guesses were correct, branches cost exactly the same as any other instruction.</para>
        <para id="id1171019838591">If the prediction was wrong, the instructions that were in process had to be can- celled, resulting in wasted time and effort. A simple branch prediction scheme is typically correct well over 90% of the time, significantly reducing the overall negative performance impact of pipeline stalls due to branches. All recent RISC designs incorporate some type of branch prediction, making branch delay slots effectively unnecessary.</para>
        <para id="id1171020516473">Another mechanism for reducing branch penalties is <emphasis effect="italics">conditional execution</emphasis>. These are instructions that look like branches in source code, but turn out to be a special type of instruction in the object code. They are very useful because they replace test and branch sequences altogether. The following lines of code capture the sense of a conditional branch:</para>
        <code id="id1171020058851" display="block"><newline/>
      IF ( B &lt; C ) THEN 
         A = D 
       ELSE
         A = E 
       ENDIF
    </code>
        <para id="id1171020246768">Using branches, this would require at least two branches to ensure that the proper value ended up in A. Using conditional execution, one might generate code that looks as follows:</para>
        <code id="id1171018373221" display="block"><newline/>
      COMPARE  B &lt; C
      IF TRUE  A = D     conditional instruction
      IF FALSE A = E     conditional instruction
    </code>
        <para id="id1171016642404">This is a sequence of three instructions with <emphasis effect="italics">no branches</emphasis>. One of the two assignments executes, and the other acts as a no-op. No branch prediction is needed, and the pipeline operates perfectly. There is a cost to taking this approach when there are a large number of instructions in one or the other branch paths that would seldom get executed using the traditional branch instruction model.</para>
      </section>
      <section id="id1171020046625">
        <title>Load/Store Architecture</title>
        <para id="id1171019676920">In a load/store instruction set architecture, memory references are limited to explicit load and store instructions. Each instruction may not make more than one memory reference per instruction. In a CISC processor, arithmetic and logical instructions can include embedded memory references. There are three reasons why limiting loads and stores to their own instructions is an improvement:</para>
        <list id="id1171020637190" list-type="bulleted">
          <item>First, we want all instructions to be the same length, for the reasons given above. However, fixed lengths impose a budget limit when it comes to describing what the operation does and which registers it uses. An instruction that both referenced memory and performed some calculation wouldn’t fit within one instruction word.</item>
          <item>Second, giving every instruction the option to reference memory would com- plicate the pipeline because there would be two computations to perform— the address calculation plus whatever the instruction is supposed to do — but there is only one execution stage. We could throw more hardware at it, but by restricting memory references to explicit loads and stores, we can avoid the problem entirely. Any instruction can perform an address calculation or some other operation, but no instruction can do both.</item>
          <item>The third reason for limiting memory references to explicit loads and stores is that they can take more time than other instructions — sometimes two or three clock cycles more. A general instruction with an embedded memory reference would get hung up in the operand fetch stage for those extra cycles, waiting for the reference to complete. Again we would be faced with an instruction pipeline stall.</item>
        </list>
        <para id="id1171020234844">Explicit load and store instructions can kick off memory references in the pipeline’s execute stage, to be completed at a later time (they might complete immediately; it depends on the processor and the cache). An operation downstream may require the result of the reference, but that’s all right, as long as it is far enough downstream that the reference has had time to complete.</para>
      </section>
      <section id="id1171019933412">
        <title>Simple Addressing Modes</title>
        <para id="id1171020564523">Just as we want to simplify the instruction set, we also want a simple set of memory addressing modes. The reasons are the same: complicated address calculations, or those that require multiple memory references, will take too much time and stall the pipeline. This doesn’t mean that your program can’t use elegant data structures; the compiler explicitly generates the extra address arithmetic when it needs it, as long as it can count on a few fundamental addressing modes in hardware. In fact, the extra address arithmetic is often easier for the compiler to optimize into faster forms (see <link document="m33787"/> and <link document="m33696" target-id="id1164354674174"/>).</para>
        <para id="id1171018537785">Of course, cutting back the number of addressing modes means that some memory references will take more real instructions than they might have taken on a CISC machine. However, because everything executes more quickly, it generally is still a performance win.</para>
      </section>
   </content>
</document>
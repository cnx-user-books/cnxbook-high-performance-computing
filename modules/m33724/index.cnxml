<document xmlns="http://cnx.rice.edu/cnxml">
  <title>Other Clutter</title>
<metadata xmlns:md="http://cnx.rice.edu/mdml">
  <md:content-id>m33724</md:content-id>
  <md:title>Other Clutter</md:title>
  <md:abstract/>
  <md:uuid>6523aa02-7b19-4a22-ae7e-9473ed322c8c</md:uuid>
</metadata>

<content>
      <para id="id10768860">Clutter comes in many forms. Consider the previous sections as having dealt with large pieces of junk you might find in the front hall closet: an ironing board, hockey sticks, and pool cues. Now we are down to the little things: a widowed checker, a tennis ball, and a hat nobody owns. We want to mention a few of them here. We apologize in advance for changing subjects a lot, but that’s the nature of cleaning out a closet!</para>
      <section id="id10768962">
        <title>Data Type Conversions</title>
        <para id="id10768968">Statements that contain runtime type conversions suffer a little performance penalty each time the statement is executed. If the statement is located in a portion of the program where there is a lot of activity, the total penalty can be significant.</para>
        <para id="id10769014">People have their reasons for writing applications with mixed typing. Often it is a matter of saving memory space, memory bandwidth, or time. In the past, for instance, double-precision calculations took twice as long as their single-precision counterparts, so if some of the calculations could be arranged to take place in single precision, there could be a performance win.<footnote id="id10769081">Nowadays, single-precision calculations can take <emphasis effect="italics">longer</emphasis> than double-precision calculations from register to register.</footnote> But any time saved by performing part of the calculations in single precision and part in double precision has to be measured against the additional overhead caused by the runtime type conversions. In the following code, the addition of <code display="inline">A(I)</code> to <code display="inline">B(I)</code> is <emphasis effect="italics">mixed type</emphasis>:</para>
        <code id="id10769160" display="block"><newline/>
      INTEGER NUMEL, I 
      PARAMETER (NUMEL = 1000) 
      REAL*8 A(NUMEL)
      REAL*4 B(NUMEL) 
      DO I=1,NUMEL
        A(I) = A(I) + B(I) 
      ENDDO
    </code>
        <para id="id10769215">In each iteration, <code display="inline">B(I)</code> has to be promoted to double precision before the addition can occur. You don’t see the promotion in the source code, but it’s there, and it takes time.</para>
        <para id="id10769250">C programmers beware: in Kernighan and Ritchie (K&amp;R) C, all floating-point calculations in C programs take place in double precision — even if all the variables involved are declared as <emphasis effect="italics">float</emphasis>. It is possible for you to write a whole K+R application in one precision, yet suffer the penalty of many type conversions.</para>
        <para id="id10767699">Another data type–related mistake is to use character operations in IF tests. On many systems, character operations have poorer performance than integer operations since they may be done via procedure calls. Also, the optimizers may not look at code using character variables as a good candidate for optimization. For example, the following code:</para>
        <code id="id10767769" display="block"><newline/>
      DO I=1,10000
        IF ( CHVAR(I) .EQ. ’Y’ ) THEN 
          A(I) = A(I) + B(I)*C
        ENDIF 
      ENDDO
    </code>
        <para id="id10767811">might be better written using an integer variable to indicate whether or not a computation should be performed:</para>
        <code id="id10767828" display="block"><newline/>
      DO I=1,10000
        IF ( IFLAG(I) .EQ. 1 ) THEN 
          A(I) = A(I) + B(I)*C
        ENDIF 
      ENDDO
    </code>
        <para id="id10767864">Another way to write the code, assuming the IFLAG variable was 0 or 1, would be as follows:</para>
        <code id="id10767877" display="block"><newline/>
      DO I=1,10000
        A(I) = A(I) + B(I)*C*IFLAG(I) 
      ENDDO
    </code>
        <para id="id10767902">The last approach might actually perform slower on some computer systems than the approach using the IF and the integer variable.</para>
      </section>
      <section id="id10767922">
        <title>Doing Your Own Common Subexpression Elimination</title>
        <para id="id10767930">So far we have given your compiler the benefit of the doubt. <emphasis effect="italics">Common subexpression elimination</emphasis> — the ability of the compiler to recognize repeated patterns in the code and replace all but one with a temporary variable — probably works on your machine for simple expressions. In the following lines of code, most compilers would recognize <code display="inline">a+b</code> as a common subexpression:</para>
        <code id="id10767999" display="block"><newline/>
      c = a + b + d 
      e = q + a + b 
    </code>
        <para id="id10768038">becomes:</para>
        <code id="id10768043" display="block"><newline/>
      temp = a + b 
      c = temp + d 
      e = q + temp
    </code>
        <para id="id10768080">Substituting for <code display="inline">a+b</code> eliminates some of the arithmetic. If the expression is reused many times, the savings can be significant. However, a compiler’s ability to recognize common subexpressions is limited, especially when there are multiple components, or their order is permuted. A compiler might not recognize that <code display="inline">a+b+c</code> and <code display="inline">c+b+a</code> are equivalent.<footnote id="id10768147">And because of overflow and round-off errors in floating-point, in some situations they might not be equivalent.</footnote> For important parts of the program, you might consider doing common subexpression elimination of complicated expressions by hand. This guarantees that it gets done. It compromises beauty somewhat, but there are some situations where it is worth it.</para>
        <para id="id10768230">Here’s another example in which the function sin is called twice with the same argument:</para>
        <code id="id10768258" display="block"><newline/>
      x = r*sin(a)*cos(b); 
      y = r*sin(a)*sin(b); 
      z = r*cos(a);
    </code>
        <para id="id10768286">becomes:</para>
        <code id="id10768290" display="block"><newline/>
      temp = r*sin(a); 
      x = temp*cos(b); 
      y = temp*sin(b); 
      z = r*cos(a);
    </code>
        <para id="id10768326">We have replaced one of the calls with a temporary variable. We agree, the savings for eliminating one transcendental function call out of five won’t win you a Nobel prize, but it does call attention to an important point: compilers typically do not perform common subexpression elimination over subroutine or function calls. The compiler can’t be sure that the subroutine call doesn’t change the state of the argument or some other variables that it can’t see.</para>
        <para id="id10768406">The only time a compiler might eliminate common subexpressions containing function calls is when they are intrinsics, as in FORTRAN. This can be done because the compiler can assume some things about their side effects. You, on the other hand, can see into subroutines, which means you are better qualified than the compiler to group together common subexpressions involving subroutines or functions.</para>
      </section>
      <section id="id10765880">
        <title>Doing Your Own Code Motion</title>
        <para id="id10765888">All of these optimizations have their biggest payback within loops because that’s where all of a program’s activity is concentrated. One of the best ways to cut down on runtime is to move unnecessary or repeated (invariant) instructions out of the main flow of the code and into the suburbs. For loops, it’s called <emphasis effect="italics">hoisting</emphasis> instructions when they are pulled out from the top and <emphasis effect="italics">sinking</emphasis> when they are pushed down below. Here’s an example:</para>
        <code id="id10766002" display="block"><newline/>
      DO I=1,N
        A(I) = A(I) / SQRT(X*X + Y*Y) 
      ENDDO
    </code>
        <para id="id10766030">becomes:</para>
        <code id="id10766034" display="block"><newline/>
      TEMP = 1 / SQRT(X*X + Y*Y) 
      DO I=1,N
        A(I) = A(I) * TEMP 
      ENDDO
    </code>
        <para id="id10766068">We hoisted an expensive, invariant operation out of the loop and assigned the result to a temporary variable. Notice, too, that we made an algebraic simplification when we exchanged a division for multiplication by an inverse. The multiplication will execute much more quickly. Your compiler might be smart enough to make these transformations itself, assuming you have instructed the compiler that these are legal transformations; but without crawling through the assembly language, you can’t be positive. Of course, if you rearrange code by hand and the runtime for the loop suddenly goes down, you will know that the compiler has been sandbagging all along.</para>
        <para id="id10766212">Sometimes you want to sink an operation below the loop. Usually, it’s some calculation performed each iteration but whose result is only needed for the last. To illustrate, here’s a sort of loop that is different from the ones we have been looking at. It searches for the final character in a character string:</para>
        <code id="id10766268" display="block"><newline/>
      while (*p != ’ ’)
        c = *p++;
    </code>
        <para id="id10766282">becomes:</para>
        <code id="id10766286" display="block"><newline/>
      while (*p++ != ’ ’);
      c = *(p-1);
    </code>
        <para id="id10766300">The new version of the loop moves the assignment of <code display="inline">c</code> beyond the last iteration. Admittedly, this transformation would be a reach for a compiler and the savings wouldn’t even be that great. But it illustrates the notion of sinking an operation very well.</para>
        <para id="id10766359">Again, hoisting or sinking instructions to get them out of loops is something your compiler should be capable of doing. But often you can slightly restructure the calculations yourself when you move them to get an even greater benefit.</para>
      </section>
      <section id="id10766406">
        <title>Handling Array Elements in Loops</title>
        <para id="id10766412">Here’s another area where you would like to trust the compiler to do the right thing. When making repeated use of an array element within a loop, you want to be charged just once for loading it from memory. Take the following loop as an example. It reuses <code display="inline">X(I)</code> twice:</para>
        <code id="id10766484" display="block"><newline/>
      DO I=1,N
        XOLD(I) = X(I)
        X(I)= X(I) + XINC(I) 
      ENDDO
    </code>
        <para id="id10766520">In reality, the steps that go into retrieving <code display="inline">X(I)</code> are just additional common subex- pressions: an address calculation (possibly) and a memory load operation. You can see that the operation is repeated by rewriting the loop slightly:</para>
        <code id="id10766544" display="block"><newline/>
      DO I=1,N
        TEMP= X(I) 
        XOLD(I) = TEMP
        X(I)= TEMP + XINC(I) 
      ENDDO
    </code>
        <para id="id10766589">FORTRAN compilers <emphasis effect="italics">should</emphasis> recognize that the same <code display="inline">X(I)</code> is being used twice and that it only needs to be loaded once, but compilers aren’t always so smart. You sometimes have to create a temporary scalar variable to hold the value of an array element over the body of a loop. This is particularly true when there are subroutine calls or functions in the loop, or when some of the variables are <code display="inline">external</code> or <code display="inline">COMMON</code>. Make sure to match the types between the temporary variables and the other variables. You don’t want to incur type conversion overhead just because you are “helping” the compiler. For C compilers, the same kind of indexed expres- sions are an even greater challenge. Consider this code:</para>
        <code id="id10772606" display="block"><newline/>
      doinc(int xold[],int x[],int xinc[],int n)
      {
        for (i=0; i&lt;n; i++) {
          xold[i] = x[i];
          x[i]= x[i] + xinc[i];
        }
      }
    </code>
        <para id="id10772654">Unless the compiler can see the definitions of <code display="inline">x</code>, <code display="inline">xinc</code>, and <code display="inline">xold</code>, it has to assume that they are pointers leading back to the same storage, and repeat the loads and stores. In this case, introducing temporary variables to hold the values <code display="inline">x</code>, <code display="inline">xinc</code>, and <code display="inline">xold</code> is an optimization the compiler wasn’t free to make.</para>
        <para id="id10772756">Interestingly, while putting scalar temporaries in the loop is useful for RISC and superscalar machines, it doesn’t help code that runs on parallel hardware. A parallel compiler looks for opportunities to eliminate the scalars or, at the very least, to replace them with temporary vectors. If you run your code on a parallel machine from time to time, you might want to be careful about introducing scalar temporary variables into a loop. A dubious performance gain in one instance could be a real performance loss in another.</para>
      </section>
  </content>
</document>